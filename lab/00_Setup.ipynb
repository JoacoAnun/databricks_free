{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e99a7cf7-26dc-464d-9f10-df21f5a3cf0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Community Edition Lab\n",
    "\n",
    "This is a lab to understand Databricks at the Community Edition level. This notebook is for the complete setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25c39807-f5ac-4bcf-aa73-8d472dbdd453",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS orders_raw;\n",
    "DROP TABLE IF EXISTS orders_cleaned;\n",
    "DROP TABLE IF EXISTS cn_daily_customer_books;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13639fee-d7b6-4724-9906-07a6d9b8ff37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Creation and usage of schema and volume for data storage\n",
    "CREATE SCHEMA IF NOT EXISTS STRATA_LAB;\n",
    "USE SCHEMA STRATA_LAB;\n",
    "CREATE VOLUME IF NOT EXISTS workspace.strata_lab.entrenamiento;\n",
    "\n",
    "DROP TABLE IF EXISTS orders_bronze;\n",
    "DROP TABLE IF EXISTS orders_enriched_temp;\n",
    "DROP TABLE IF EXISTS orders_updates;\n",
    "DROP TABLE IF EXISTS orders_raw;\n",
    "DROP TABLE IF EXISTS orders_cleaned;\n",
    "DROP TABLE IF EXISTS cn_daily_customer_books;\n",
    "DROP TABLE IF EXISTS customers_lookup;\n",
    "DROP TABLE IF EXISTS orders;\n",
    "DROP TABLE IF EXISTS orders_raw;\n",
    "DROP TABLE IF EXISTS orders_silver;\n",
    "DROP TABLE IF EXISTS books;\n",
    "DROP TABLE IF EXISTS customers;\n",
    "DROP FUNCTION IF EXISTS get_url;\n",
    "DROP FUNCTION IF EXISTS site_type;\n",
    "\n",
    "CREATE WIDGET TEXT book_store_path DEFAULT '/Volumes/workspace/strata_lab/entrenamiento/book_store';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a56bffe5-da62-4fa0-b530-09ab9fcd7fbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Installation of required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c57c69f-07d3-48c8-907e-5546cd0b009b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Install required libraries and restart Python environment\n",
    "%pip install dbldatagen faker\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6c251a6-e61b-47ee-9ae5-6cc4e4771902",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve the book store path from the Databricks widget\n",
    "book_store_path = dbutils.widgets.get(\"book_store_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aaa9d23-fcd0-4da8-850b-33042077b6a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    row_count=1000,\n",
    "    save_path=\"/Volumes/workspace/strata_lab/entrenamiento/book_store/orders/\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates synthetic order data and saves it in Parquet format.\n",
    "    Randomness is introduced so that ~20% of orders contain multiple books.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import (\n",
    "        col,\n",
    "        format_string,\n",
    "        lit,\n",
    "        struct,\n",
    "        array,\n",
    "        to_json,\n",
    "        unix_timestamp,\n",
    "        when,\n",
    "        rand,\n",
    "        sequence,\n",
    "        explode,\n",
    "        element_at,\n",
    "        shuffle,\n",
    "        collect_list,\n",
    "        sum as _sum,\n",
    "        count,\n",
    "        first,\n",
    "        floor,\n",
    "    )\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "    books_schema = StructType([\n",
    "        StructField(\"book_id_map\", StringType(), False),\n",
    "        StructField(\"price\", IntegerType(), False)\n",
    "    ])\n",
    "    from dbldatagen import DataGenerator\n",
    "\n",
    "    # List of available books and their prices\n",
    "    books_data = [\n",
    "        (\"B01\", 25), (\"B02\", 28), (\"B03\", 30), (\"B04\", 15),\n",
    "        (\"B05\", 40), (\"B06\", 22), (\"B07\", 18), (\"B08\", 41),\n",
    "        (\"B09\", 50), (\"B10\", 12), (\"B11\", 19), (\"B12\", 29),\n",
    "    ]\n",
    "    book_ids = [b[0] for b in books_data]\n",
    "        \n",
    "    books_df = spark.createDataFrame(books_data, schema=books_schema)\n",
    "\n",
    "    # Generate synthetic orders with random customer and order info\n",
    "    orders_gen = (\n",
    "        DataGenerator(spark, name=\"orders_spec\", rows=row_count, partitions=4)\n",
    "        .withColumn(\"order_id_int\", \"long\", minValue=6341, uniqueValues=row_count)\n",
    "        .withColumn(\n",
    "            \"order_timestamp_dt\",\n",
    "            \"timestamp\",\n",
    "            begin=\"2022-07-01 00:00:00\",\n",
    "            end=\"2022-07-31 23:59:59\",\n",
    "            random=True,\n",
    "        )\n",
    "        .withColumn(\"customer_id_int\", \"int\", minValue=1, maxValue=10000, random=True)\n",
    "        .withColumn(\"multi_book_rand\", \"float\", minValue=0.0, maxValue=1.0)\n",
    "    )\n",
    "    orders_df = orders_gen.build()\n",
    "\n",
    "    # Assign number of books per order (~20% have multiple books)\n",
    "    orders_with_book_count = orders_df.withColumn(\n",
    "        \"num_books\",\n",
    "        when(col(\"multi_book_rand\") >= 0.8, (rand() * 3 + 2).cast(\"int\"))\n",
    "        .otherwise(lit(1)),\n",
    "    )\n",
    "    \n",
    "    # Create a sequence for each book in the order\n",
    "    line_items_unassigned = orders_with_book_count.withColumn(\n",
    "        \"book_seq_array\", sequence(lit(1), col(\"num_books\"))\n",
    "    ).select(\"*\", explode(col(\"book_seq_array\")).alias(\"book_seq\"))\n",
    "\n",
    "    # Randomly assign a book_id to each line item\n",
    "    book_ids_col = array([lit(b) for b in book_ids])\n",
    "    line_items_with_book = line_items_unassigned.withColumn(\n",
    "        \"book_id\", element_at(shuffle(book_ids_col), 1)\n",
    "    )\n",
    "\n",
    "    # Assign a random quantity (1-4) for each book\n",
    "    line_items_with_quantity = line_items_with_book.withColumn(\n",
    "        \"quantity\", (floor(rand() * 4) + 1).cast(\"int\")\n",
    "    )\n",
    "\n",
    "    # Join with books_df to get the price for each book\n",
    "    df_with_price = line_items_with_quantity.join(\n",
    "        books_df,\n",
    "        line_items_with_quantity[\"book_id\"] == books_df[\"book_id_map\"],\n",
    "        \"left\"\n",
    "    ).withColumn(\n",
    "        \"price\", col(\"price\").cast(\"int\")\n",
    "    )\n",
    "\n",
    "    # Create a struct for each book item in the order\n",
    "    df_with_struct = df_with_price.withColumn(\n",
    "        \"book_item\",\n",
    "        struct(\n",
    "            col(\"book_id\"),\n",
    "            col(\"quantity\"),\n",
    "            (col(\"price\") * col(\"quantity\")).alias(\"subtotal\"),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Aggregate line items by order_id to form the final order\n",
    "    df_aggregated = df_with_struct.groupBy(\"order_id_int\").agg(\n",
    "        first(\"order_timestamp_dt\").alias(\"order_timestamp_dt\"),\n",
    "        first(\"customer_id_int\").alias(\"customer_id_int\"),\n",
    "        _sum(col(\"price\") * col(\"quantity\")).alias(\"total\"),\n",
    "        _sum(\"quantity\").alias(\"quantity\"),\n",
    "        collect_list(\"book_item\").alias(\"books\"),\n",
    "    )\n",
    "\n",
    "    # Format columns and select final fields for output\n",
    "    df_final = (\n",
    "        df_aggregated\n",
    "        .withColumn(\"order_id\", format_string(\"%016d\", col(\"order_id_int\")))\n",
    "        .withColumn(\"order_timestamp\", unix_timestamp(col(\"order_timestamp_dt\")))\n",
    "        .withColumn(\"customer_id\", format_string(\"C%05d\", col(\"customer_id_int\")))\n",
    "        .select(\n",
    "            \"order_id\", \"order_timestamp\", \"customer_id\", \"quantity\", \"total\", \"books\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Save the generated orders as Parquet files\n",
    "    df_final.write.mode(\"append\").format(\"parquet\").save(\n",
    "        f\"{save_path}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76bbf536-cf18-402a-a642-fb19a8a61e26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_customer_data(from_scratch=False, nr_new_customers=200):\n",
    "    \"\"\"\n",
    "    Reads the unique customer_id values from the 'orders_bronze' table, generates fake personal data for each one, and saves them in a new location.\n",
    "    \"\"\"\n",
    "    from pyspark.sql.functions import (\n",
    "        col, lit, struct, to_json, rand, expr,\n",
    "        monotonically_increasing_id, regexp_extract,\n",
    "        max as _max, array, element_at, floor\n",
    "    )\n",
    "    from dbldatagen import DataGenerator, FakerTextFactory\n",
    "\n",
    "    # List of email domains for generating fake emails\n",
    "    email_domains = [\n",
    "        \"example.com\", \"gmail.com\", \"hotmail.com\", \"yahoo.com\", \"outlook.com\",\n",
    "        \"gmil.com\", \"t-online.de\", \"university.edu\", \"protonmail.com\", \"icloud.com\",\n",
    "        \"fastmail.com\", \"zoho.com\", \"gmx.net\", \"web.de\", \"mail.com\"\n",
    "    ]\n",
    "    email_domains_col = array([lit(d) for d in email_domains])\n",
    "\n",
    "    if from_scratch:\n",
    "        try:\n",
    "            # Read existing orders to determine the max customer_id\n",
    "            orders_df = spark.read.parquet(\"/Volumes/workspace/strata_lab/entrenamiento/book_store/orders/\")\n",
    "            max_id_str = orders_df.select(\n",
    "                _max(regexp_extract(col(\"customer_id\"), r'C(\\d+)', 1).cast(\"int\")).alias(\"max_id\")\n",
    "            ).collect()[0][\"max_id\"]\n",
    "            start_id = max_id_str + 1 if max_id_str is not None else 1\n",
    "        except Exception as e:\n",
    "            # If reading fails, start from 1\n",
    "            start_id = 1\n",
    "\n",
    "        faker_factory = FakerTextFactory(locale=[\"en_US\"])\n",
    "\n",
    "        # Generate base customer profile data (without email)\n",
    "        customer_profile_gen = (\n",
    "            DataGenerator(\n",
    "                sparkSession=spark,\n",
    "                name=\"customer_profile_spec\",\n",
    "                rows=nr_new_customers,\n",
    "                partitions=4,\n",
    "            )\n",
    "            .withColumn(\"starting_id\", \"long\", minValue=start_id, maxValue=start_id + nr_new_customers - 1, uniqueValues=nr_new_customers)\n",
    "            .withColumn(\"customer_id\", expr=\"format_string('C%05d', starting_id)\")\n",
    "            .withColumn(\"first_name\", text=faker_factory(\"first_name\"))\n",
    "            .withColumn(\"last_name\", text=faker_factory(\"last_name\"))\n",
    "            .withColumn(\"gender\", \"string\", values=[\"Male\", \"Female\", \"Non-binary\"])\n",
    "            .withColumn(\"street\", text=faker_factory(\"street_address\"))\n",
    "            .withColumn(\"city\", text=faker_factory(\"city\"))\n",
    "            .withColumn(\"country\", \"string\", values=[\"USA\"])\n",
    "            .withColumn(\"updated\", \"date\", uniqueValues=365, random=True)\n",
    "        )\n",
    "\n",
    "        df_generated_profiles = customer_profile_gen.build()\n",
    "\n",
    "        # Add email column using random domain and names\n",
    "        df_with_email = (\n",
    "            df_generated_profiles\n",
    "            .withColumn(\"email_domain\", element_at(email_domains_col, (floor(rand() * len(email_domains)) + 1).cast(\"int\")))\n",
    "            .withColumn(\"email\", expr(\"concat(lower(first_name), '.', lower(last_name), '@', email_domain)\"))\n",
    "        )\n",
    "\n",
    "        # Create JSON profile structure\n",
    "        customers = df_with_email.withColumn(\n",
    "            \"profile\",\n",
    "            to_json(\n",
    "                struct(\n",
    "                    col(\"first_name\"),\n",
    "                    col(\"last_name\"),\n",
    "                    col(\"gender\"),\n",
    "                    struct(col(\"street\"), col(\"city\"), col(\"country\")).alias(\"address\"),\n",
    "                )\n",
    "            )\n",
    "        ).select(\"customer_id\", \"email\", \"profile\", \"updated\")\n",
    "\n",
    "        # Save generated customer data as JSON\n",
    "        customers.repartition(6).write.mode(\"append\").format(\"json\").save(\n",
    "            \"/Volumes/workspace/strata_lab/entrenamiento/book_store/customers-json-new/\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            # Read unique customer IDs from orders\n",
    "            customers_df = spark.read.parquet(\"/Volumes/workspace/strata_lab/entrenamiento/book_store/orders/\").select(\"customer_id\").distinct()\n",
    "        except Exception as e:\n",
    "            # Fallback: generate dummy customer IDs if reading fails\n",
    "            customers_df = spark.range(1, 200).selectExpr(\"format_string('C%05d', id) as customer_id\")\n",
    "\n",
    "        customer_count = customers_df.count()\n",
    "        if customer_count == 0:\n",
    "            return\n",
    "\n",
    "        faker_factory = FakerTextFactory(locale=[\"en_US\"])\n",
    "\n",
    "        # Generate base customer profile data (without email)\n",
    "        customer_profile_gen = (\n",
    "            DataGenerator(\n",
    "                sparkSession=spark,\n",
    "                name=\"customer_profile_spec\",\n",
    "                baseDF=customers_df,\n",
    "                rows=customer_count,\n",
    "                partitions=4,\n",
    "            )\n",
    "            .withColumn(\"first_name\", text=faker_factory(\"first_name\"))\n",
    "            .withColumn(\"last_name\", text=faker_factory(\"last_name\"))\n",
    "            .withColumn(\"gender\", \"string\", values=[\"Male\", \"Female\", \"Non-binary\"])\n",
    "            .withColumn(\"street\", text=faker_factory(\"street_address\"))\n",
    "            .withColumn(\"city\", text=faker_factory(\"city\"))\n",
    "            .withColumn(\"country\", \"string\", values=[\"USA\"])\n",
    "            .withColumn(\"updated\", \"date\", uniqueValues=365, random=True)\n",
    "        )\n",
    "\n",
    "        df_generated_profiles = customer_profile_gen.build()\n",
    "\n",
    "        # Add join key to preserve customer_id after DataGenerator\n",
    "        customers_df_with_key = customers_df.withColumn(\"join_key\", monotonically_increasing_id())\n",
    "        profiles_with_key = df_generated_profiles.withColumn(\"join_key\", monotonically_increasing_id())\n",
    "        df_merged = customers_df_with_key.join(profiles_with_key, \"join_key\").drop(\"join_key\")\n",
    "\n",
    "        # Add email column using random domain and names\n",
    "        df_with_email = (\n",
    "            df_merged\n",
    "            .withColumn(\"email_domain\", element_at(email_domains_col, (floor(rand() * len(email_domains)) + 1).cast(\"int\")))\n",
    "            .withColumn(\"email\", expr(\"concat(lower(first_name), '.', lower(last_name), '@', email_domain)\"))\n",
    "        )\n",
    "        \n",
    "        # Create JSON profile structure\n",
    "        customers = df_with_email.withColumn(\n",
    "            \"profile\",\n",
    "            to_json(\n",
    "                struct(\n",
    "                    col(\"first_name\"),\n",
    "                    col(\"last_name\"),\n",
    "                    col(\"gender\"),\n",
    "                    struct(col(\"street\"), col(\"city\"), col(\"country\")).alias(\"address\"),\n",
    "                )\n",
    "            )\n",
    "        ).select(\"customer_id\", \"email\", \"profile\", \"updated\")\n",
    "\n",
    "        # Save generated customer data as JSON\n",
    "        customers.repartition(6).write.mode(\"overwrite\").format(\"json\").save(\n",
    "            \"/Volumes/workspace/strata_lab/entrenamiento/book_store/customers/\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc8ac3e4-7eb6-41b3-a9ca-2e6d482373a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clear_workspace():\n",
    "    \"\"\"\n",
    "    Function to delete the generated files and folders\n",
    "    \"\"\"\n",
    "    # Remove the entire training volume directory and its contents\n",
    "    dbutils.fs.rm(\"/Volumes/workspace/strata_lab/entrenamiento/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48a56751-ff80-49f8-ab97-71610a66dfed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "# Output directory for generated CSV files\n",
    "output_dir = \"/Volumes/workspace/strata_lab/entrenamiento/book_store/books-csv\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "fake = Faker()\n",
    "def generate_books():\n",
    "    # List of possible book categories\n",
    "    categories = [\n",
    "    \"Computer Science\", \"Mathematics\", \"Physics\",\n",
    "    \"Philosophy\", \"Literature\", \"History\", \"Art\"\n",
    "    ]\n",
    "    books = []\n",
    "    # Generate 12 fake books with random data\n",
    "    for i in range(1, 13):\n",
    "        book = {\n",
    "            \"book_id\": f\"B{i:02d}\",\n",
    "            \"title\": fake.sentence(nb_words=4).replace(\",\", \"\"), \n",
    "            \"author\": fake.name(),\n",
    "            \"category\": random.choice(categories),            \n",
    "            \"price\": fake.random_int(min=10, max=100)\n",
    "        }\n",
    "        books.append(book)\n",
    "\n",
    "    # Split books into 3 CSV files, 4 books each\n",
    "    for file_idx in range(3):\n",
    "        start = file_idx * 4\n",
    "        end = start + 4\n",
    "        chunk = books[start:end]\n",
    "\n",
    "        file_path = os.path.join(output_dir, f\"books_part_{file_idx+1}.csv\")\n",
    "\n",
    "        # Write chunk to CSV file with semicolon delimiter\n",
    "        with open(file_path, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=[\"book_id\", \"title\", \"author\", \"category\", \"price\"], delimiter=\";\")\n",
    "            writer.writeheader()\n",
    "            writer.writerows(chunk)\n",
    "\n",
    "    # Also generate table for SQL querying\n",
    "    from pyspark.sql.functions import col\n",
    "    spark.sql(\"DROP TABLE IF EXISTS books\")  # Drop table if it exists\n",
    "\n",
    "    # Read all generated CSV files into a Spark DataFrame\n",
    "    books_df = spark.read.option(\"header\", \"true\") \\\n",
    "        .option(\"delimiter\", \";\") \\\n",
    "    .csv(f\"{dbutils.widgets.get('book_store_path')}/books-csv\")\n",
    "\n",
    "    # Cast price column to double type\n",
    "    books_df = books_df.withColumn(\"price\", col(\"price\").cast(\"double\"))\n",
    "\n",
    "    # Save DataFrame as a Spark SQL table\n",
    "    books_df.write.saveAsTable(\"books\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea688ed0-8b8b-43ca-a76b-64d132456d3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "def new_books_csv():\n",
    "\n",
    "    output_dir = \"/Volumes/workspace/strata_lab/entrenamiento/book_store/books-csv-new\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Define the header and the rows of data\n",
    "    header = ['book_id', 'title', 'author', 'category', 'price']\n",
    "    data = [\n",
    "        ['B14', 'Data Communications and Networking', 'Behrouz A. Forouzan', 'Computer Science', 34],\n",
    "        ['B15', 'Inside the Java Virtual Machine', 'Bill Venners', 'Computer Science', 41],\n",
    "        ['B13', 'Linux pocket guide', 'Daniel J. Barrett', 'Computer Science', 26],\n",
    "        ['B16', 'Green for Life', 'Victoria Boutenko', 'Food', 18],\n",
    "        ['B17', 'Cooking with Love', 'Carla Hall', 'Food', 23]\n",
    "    ]\n",
    "\n",
    "    # Specify the filename\n",
    "    file_path = os.path.join(output_dir, \"books.csv\")\n",
    "    # Write the data to the CSV file\n",
    "    with open(file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        # Write the data rows\n",
    "        writer.writerows(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "027adfd6-8ec4-4a4d-b760-55fd417d328b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_books_cdc():\n",
    "    import json\n",
    "\n",
    "    # Creamos las carpetas a utilizar dentro del volumen de entrenamiento\n",
    "    # Buscamos las carpetas en el volumen\n",
    "    dbutils.fs.mkdirs(\"/Volumes/workspace/strata_lab/entrenamiento/books-cdc/\")\n",
    "\n",
    "    # Data structured as a list of dictionaries, representing the rows from the table.\n",
    "    # SQL NULL values are represented as None in Python, which translates to null in JSON.\n",
    "    book_data = [\n",
    "        {\n",
    "            \"id\": 1,\n",
    "            \"author\": \"Addy Osmani\",\n",
    "            \"book_id\": \"B02\",\n",
    "            \"category\": \"Computer Science\",\n",
    "            \"price\": 40,\n",
    "            \"row_status\": \"UPDATE\",\n",
    "            \"row_time\": \"2022-11-05T17:50:23.601Z\",\n",
    "            \"title\": \"Learning JavaScript Design Patterns\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 2,\n",
    "            \"author\": \"Tariq Rashid\",\n",
    "            \"book_id\": \"B03\",\n",
    "            \"category\": \"Computer Science\",\n",
    "            \"price\": 30,\n",
    "            \"row_status\": \"UPDATE\",\n",
    "            \"row_time\": \"2022-11-05T10:11:33.507Z\",\n",
    "            \"title\": \"Make Your Own Neural Network\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 3,\n",
    "            \"author\": \"null\",\n",
    "            \"book_id\": \"B01\",\n",
    "            \"category\": \"null\",\n",
    "            \"price\": \"null\",\n",
    "            \"row_status\": \"DELETE\",\n",
    "            \"row_time\": \"2022-11-05T17:50:23.601Z\",\n",
    "            \"title\": \"null\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 4,\n",
    "            \"author\": \"Mark W. Spong\",\n",
    "            \"book_id\": \"B04\",\n",
    "            \"category\": \"Computer Science\",\n",
    "            \"price\": 20,\n",
    "            \"row_status\": \"INSERT\",\n",
    "            \"row_time\": \"2022-11-05T11:12:05.419Z\",\n",
    "            \"title\": \"Robot Dynamics and Control\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 5,\n",
    "            \"author\": \"Luciano Ramalho\",\n",
    "            \"book_id\": \"B05\",\n",
    "            \"category\": \"Computer Science\",\n",
    "            \"price\": 47,\n",
    "            \"row_status\": \"INSERT\",\n",
    "            \"row_time\": \"2022-11-05T11:12:05.419Z\",\n",
    "            \"title\": \"Fluent Python\"\n",
    "        },\n",
    "        {\n",
    "            \"id\": 6,\n",
    "            \"author\": \"François Chollet\",\n",
    "            \"book_id\": \"B06\",\n",
    "            \"category\": \"Computer Science\",\n",
    "            \"price\": 22,\n",
    "            \"row_status\": \"INSERT\",\n",
    "            \"row_time\": \"2022-11-05T11:12:05.419Z\",\n",
    "            \"title\": \"Deep Learning with Python\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Define the output filename\n",
    "    output_filename = 'book_data.json'\n",
    "\n",
    "    # Write the data to the JSON file with indentation for readability\n",
    "    try:\n",
    "        with open(\"/Volumes/workspace/strata_lab/entrenamiento/books-cdc/02.json\", 'w') as json_file:\n",
    "            for book in book_data:\n",
    "                json_file.write(f\"{book}\\n\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing to file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec60c338-9426-47b2-b0e3-3572bc1ba7a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Primera carga inical\n",
    "clear_workspace()\n",
    "load_data()\n",
    "load_data(700, save_path=book_store_path+'/orders_new')\n",
    "generate_customer_data()\n",
    "generate_customer_data(from_scratch=True)\n",
    "generate_books()\n",
    "new_books_csv()\n",
    "add_books_cdc()\n",
    "print(\"Workspace fully generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f93ce4ee-2470-43e3-b34b-63bddfdf92fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13eee9ce-a30a-4366-9078-f425940ca2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "00_Setup",
   "widgets": {
    "book_store_path": {
     "currentValue": "/Volumes/workspace/strata_lab/entrenamiento/book_store",
     "nuid": "06e6e7d8-2ffb-4320-9206-9e764362fd1d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/workspace/strata_lab/entrenamiento/book_store",
      "label": null,
      "name": "book_store_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/workspace/strata_lab/entrenamiento/book_store",
      "label": null,
      "name": "book_store_path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
